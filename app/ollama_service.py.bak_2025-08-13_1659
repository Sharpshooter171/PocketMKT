import os
import requests
from typing import Any, Dict, Optional

"""
Cliente HTTP para chamar a LLM no servidor da GPU.

Produção (CPU -> GPU na mesma VPC):
  - Use IP PRIVADO: 172.31.18.20 (porta 8000)

Testes a partir do seu notebook (internet -> GPU):
  - Use IP PÚBLICO: 98.87.41.241 (porta 8000 liberada para seu IP /32)

Variáveis de ambiente:
  LLM_HOST   (default: 172.31.18.20)
  LLM_PORT   (default: 8000)
  LLM_ROUTE  (default: /infer)
  LLM_BASE_URL (opcional; se definir, substitui host:port. Ex: http://98.87.41.241:8000)
"""

# Carrega host/porta/rota
HOST = os.getenv("LLM_HOST", "172.31.18.20")   # DEFAULT: privado (produção)
PORT = os.getenv("LLM_PORT", "8000")
ROUTE = os.getenv("LLM_ROUTE", "/infer")

# Se LLM_BASE_URL estiver definido, prevalece sobre HOST/PORT
BASE = os.getenv("LLM_BASE_URL", f"http://{HOST}:{PORT}")
OLLAMA_API_URL = f"{BASE}{ROUTE}"

# Sessão com keep-alive
SESSION = requests.Session()
SESSION.headers.update({"Content-Type": "application/json"})

def health(timeout: tuple[int, int] = (2, 5)) -> Dict[str, Any]:
    """Ping simples do servidor (GET /health)."""
    url = f"{BASE}/health"
    try:
        r = SESSION.get(url, timeout=timeout)
        if r.ok:
            try:
                return r.json()
            except Exception:
                return {"ok": True, "status_code": r.status_code}
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        raise RuntimeError(f"Falha no healthcheck em {url}: {e}") from e

def infer_llm(
    prompt: str,
    system: str = "",
    max_tokens: int = 256,
    temperature: float = 0.2,
    extra: Optional[Dict[str, Any]] = None,
    timeout_connect: int = 5,
    timeout_read: int = 60,
) -> Dict[str, Any]:
    """Chama a rota /infer da LLM."""
    payload: Dict[str, Any] = {
        "prompt": prompt,
        "system": system,
        "max_tokens": max_tokens,
        "temperature": temperature,
    }
    if extra:
        payload.update(extra)

    try:
        r = SESSION.post(OLLAMA_API_URL, json=payload, timeout=(timeout_connect, timeout_read))
        r.raise_for_status()
        return r.json()
    except requests.exceptions.ConnectTimeout as e:
        raise RuntimeError(
            f"Timeout ao conectar em {OLLAMA_API_URL}. "
            f"Verifique SG da GPU (porta 8000), HOST={HOST}, e se o serviço está em 0.0.0.0:{PORT}."
        ) from e
    except requests.exceptions.ReadTimeout as e:
        raise RuntimeError(
            f"Conectou mas não respondeu a tempo (read timeout) em {OLLAMA_API_URL}. "
            f"Verifique logs da LLM/latência."
        ) from e
    except requests.exceptions.ConnectionError as e:
        raise RuntimeError(
            f"Erro de conexão com {OLLAMA_API_URL}. Verifique rota, DNS, serviço e rede."
        ) from e
