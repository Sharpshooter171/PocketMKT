{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z9NbqPdxCqW"
      },
      "outputs": [],
      "source": [
        "# PocketMKT: Guia Prático para Rodar LLM LoRA/PEFT na AWS GPU ou Nuvem\n",
        "\n",
        "## 1. Configuração Inicial\n",
        "\n",
        "```bash\n",
        "# (RODE NO TERMINAL LOCAL)\n",
        "# Conectar por SSH\n",
        "ssh -i pocketmkt-gpu.pem ubuntu@SEU_IP_PUBLICO_AQUI\n",
        "\n",
        "# Subir arquivo do modelo já treinado (.zip)\n",
        "scp -i pocketmkt-gpu.pem /caminho/local/fine_tuned_advocacia.zip ubuntu@SEU_IP_PUBLICO_AQUI:/home/ubuntu/\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Instale as Bibliotecas Necessárias\n",
        "\n",
        "```bash\n",
        "pip install torch transformers peft accelerate huggingface_hub flask\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TvXi1E5ZxQQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. (Opcional) Ative o Ambiente PyTorch\n",
        "\n",
        "```bash\n",
        "source /opt/pytorch/bin/activate\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hjBPayQOxUaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Configure Cache/Offload para Modelos Grandes (se necessário)\n",
        "\n",
        "```bash\n",
        "mkdir -p /opt/dlami/nvme/hf_cache\n",
        "mkdir -p /opt/dlami/nvme/offload\n",
        "chmod 777 /opt/dlami/nvme/hf_cache /opt/dlami/nvme/offload\n",
        "export HF_HOME=/opt/dlami/nvme/hf_cache\n",
        "export TRANSFORMERS_CACHE=/opt/dlami/nvme/hf_cache\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HxWVoH2cxYhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5. Login Hugging Face (para modelos gated)\n",
        "\n",
        "```bash\n",
        "~/.local/bin/huggingface-cli login\n",
        "# ou, se preferir:\n",
        "pip install hfcli\n",
        "hf auth login\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Rtnnz-iUxZRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Descompacte o Modelo\n",
        "\n",
        "```bash\n",
        "unzip fine_tuned_advocacia.zip\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "xBHEpThXxbLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 7. Teste o Carregamento do Modelo LoRA/PEFT\n",
        "\n",
        "```python\n",
        "import os\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "os.environ[\"HF_HOME\"] = \"/opt/dlami/nvme/hf_cache\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/opt/dlami/nvme/hf_cache\"\n",
        "\n",
        "adapter_path = \"content/fine_tuned_advocacia\"\n",
        "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
        "base_model = peft_config.base_model_name_or_path\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"/opt/dlami/nvme/offload\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    adapter_path,\n",
        "    offload_folder=\"/opt/dlami/nvme/offload\"\n",
        ")\n",
        "\n",
        "prompt = \"Olá, advogado! Como posso te ajudar?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=64)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Sn7Ejn3ExdxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. Suba um Servidor Flask para API HTTP\n",
        "\n",
        "```python\n",
        "from flask import Flask, request, jsonify\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch, os\n",
        "\n",
        "os.environ[\"HF_HOME\"] = \"/opt/dlami/nvme/hf_cache\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/opt/dlami/nvme/hf_cache\"\n",
        "\n",
        "adapter_path = \"content/fine_tuned_advocacia\"\n",
        "peft_config = PeftConfig.from_pretrained(adapter_path)\n",
        "base_model = peft_config.base_model_name_or_path\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, torch_dtype=torch.float16, device_map=\"auto\", offload_folder=\"/opt/dlami/nvme/offload\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = PeftModel.from_pretrained(\n",
        "    model, adapter_path, offload_folder=\"/opt/dlami/nvme/offload\"\n",
        ")\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/infer', methods=['POST'])\n",
        "def infer():\n",
        "    prompt = request.json.get(\"prompt\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return jsonify({\"response\": response})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host=\"0.0.0.0\", port=8000)\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pCJvrzyaxmpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 9. Libere a porta 8000 no Security Group da AWS\n",
        "- No painel EC2 > Security Groups > Adicione regra para porta 8000 (seu IP)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6tG77LW8xnpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 10. Faça request da sua máquina local\n",
        "\n",
        "```python\n",
        "import requests\n",
        "url = \"http://SEU_IP_AWS:8000/infer\"\n",
        "data = {\"prompt\": \"Olá, advogado! Como posso te ajudar?\"}\n",
        "resp = requests.post(url, json=data)\n",
        "print(resp.json())\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dlyd5jpKxqfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 11. Dicas Rápidas\n",
        "- Monitore uso com `df -h`, `top`, `nvidia-smi`\n",
        "- Limpe caches antigos com `rm -rf /opt/dlami/nvme/hf_cache/*`\n",
        "- Sempre desligue a instância AWS para não pagar desnecessário!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Sf0P9P_yxvci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 12. Problemas comuns\n",
        "- Erro \"No space left on device\": Libere espaço e use disco temporário se possível\n",
        "- Erro de login Hugging Face: Relogue e aceite os termos do modelo\n",
        "- Erro de permissão: use `chmod 777` nos diretórios temporários para prototipagem"
      ],
      "metadata": {
        "id": "wr_LOIaZx0xX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}